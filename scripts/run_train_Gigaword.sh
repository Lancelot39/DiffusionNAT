python -m torch.distributed.launch --nproc_per_node=8 --master_port=12233 --use_env run_train.py --diff_steps 2000 --lr 0.00005 --learning_steps 200000 --save_interval 10000 --seed 102 --noise_schedule sqrt --hidden_dim 32 --bsz 128 --microbatch 128 --dataset Gigaword --data_dir datasets/Gigaword --vocab roberta --seq_len 64 --src_seq_len 128 --schedule_sampler lossaware --notes Gigaword
